



from datetime import datetime
from typing import Dict, List, Any
import re


class ReportGenerator:

    
    def __init__(self):
        self.report_template = """# {title}

**Generated:** {timestamp}

---

## üìã Question

{question}

---

## üéØ Answer

{answer}

---

## üìä Execution Summary

- **Success:** {success}
- **Iterations:** {iterations}
- **Tool Calls:** {tool_calls}
- **Actions:** {actions}
- **Memory Folds:** {memory_folds}
- **Total Tokens:** {total_tokens}

---

## üîç Reasoning Process

{reasoning_process}

---

## üõ†Ô∏è Tool Usage

{tool_usage}

---

## üìà Performance Metrics

{performance_metrics}

---

## üí≠ Full Reasoning Trace

```
{full_reasoning}
```

---

**Report generated by WebSearcher Agent**
"""
    
    def generate_report(
        self,
        question: str,
        result: Dict[str, Any],
        question_id: str = None,
        metadata: Dict = None
    ) -> str:












                        
        title = self._generate_title(question, question_id)
        
                          
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
                        
        answer = result.get('answer', 'No answer found')
        if answer and len(answer) > 500:
            answer = answer[:500] + "..."
        
                                  
        reasoning_process = self._format_reasoning_process(result)
        
                           
        tool_usage = self._format_tool_usage(result)
        
                                    
        performance_metrics = self._format_performance_metrics(result, metadata)
        
                            
        full_reasoning = result.get('reasoning', 'No reasoning trace available')
        
                       
        report = self.report_template.format(
            title=title,
            timestamp=timestamp,
            question=question,
            answer=answer,
            success="‚úÖ Yes" if result.get('success') else "‚ùå No",
            iterations=result.get('iterations', 0),
            tool_calls=result.get('tool_calls', 0),
            actions=result.get('actions', 0),
            memory_folds=result.get('memory_folds', 0),
            total_tokens=result.get('total_tokens', 0),
            reasoning_process=reasoning_process,
            tool_usage=tool_usage,
            performance_metrics=performance_metrics,
            full_reasoning=full_reasoning
        )
        
        return report
    
    def _generate_title(self, question: str, question_id: str = None) -> str:

                                           
        title = question[:100].strip()
        if len(question) > 100:
            title += "..."
        
        if question_id:
            title = f"Question {question_id}: {title}"
        
        return title
    
    def _format_reasoning_process(self, result: Dict) -> str:

        interactions = result.get('interactions', [])
        
        if not interactions:
            return "No detailed interaction log available."
        
        process = []
        for i, interaction in enumerate(interactions, 1):
            if interaction['type'] == 'tool_call':
                tool_name = interaction.get('tool_name', 'unknown')
                args = interaction.get('arguments', {})
                result_preview = interaction.get('result', '')[:200]
                
                process.append(f"### Step {i}: {tool_name}")
                process.append(f"**Arguments:** `{args}`")
                process.append(f"**Result Preview:** {result_preview}...")
                process.append("")
        
        return "\n".join(process) if process else "No tool calls recorded."
    
    def _format_tool_usage(self, result: Dict) -> str:

        interactions = result.get('interactions', [])
        
        if not interactions:
            return "No tool usage data available."
        
                          
        tool_counts = {}
        for interaction in interactions:
            if interaction['type'] == 'tool_call':
                tool_name = interaction.get('tool_name', 'unknown')
                tool_counts[tool_name] = tool_counts.get(tool_name, 0) + 1
        
                         
        usage = ["| Tool | Count |", "|------|-------|"]
        for tool, count in sorted(tool_counts.items(), key=lambda x: x[1], reverse=True):
            usage.append(f"| {tool} | {count} |")
        
        return "\n".join(usage)
    
    def _format_performance_metrics(self, result: Dict, metadata: Dict = None) -> str:

        metrics = []
        
                       
        iterations = result.get('iterations', 0)
        tool_calls = result.get('tool_calls', 0)
        tokens = result.get('total_tokens', 0)
        
        metrics.append(f"- **Efficiency:** {tool_calls / iterations if iterations > 0 else 0:.2f} tools per iteration")
        metrics.append(f"- **Token Usage:** {tokens:,} tokens")
        
        if tokens > 0 and iterations > 0:
            metrics.append(f"- **Avg Tokens/Iteration:** {tokens / iterations:.0f}")
        
                                   
        if metadata:
            if 'difficulty' in metadata:
                metrics.append(f"- **Difficulty:** {metadata['difficulty']}")
            if 'expected_iterations' in metadata:
                expected = metadata['expected_iterations']
                actual = iterations
                metrics.append(f"- **Iteration Efficiency:** {actual}/{expected} ({actual/expected*100:.0f}%)")
        
        return "\n".join(metrics)
    
    def generate_batch_summary(
        self,
        results: List[Dict],
        output_path: str = None
    ) -> str:










        summary = f"""# Batch Evaluation Summary

**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Total Questions:** {len(results)}

---

## üìä Overall Statistics

"""
        
                              
        total = len(results)
        successful = sum(1 for r in results if r.get('Success', False))
        total_iterations = sum(r.get('Iterations', 0) for r in results)
        total_tool_calls = sum(r.get('Tool_Calls', 0) for r in results)
        total_tokens = sum(r.get('Total_Tokens', 0) for r in results)
        
        summary += f"""- **Success Rate:** {successful}/{total} ({successful/total*100:.1f}%)
- **Avg Iterations:** {total_iterations/total:.1f}
- **Avg Tool Calls:** {total_tool_calls/total:.1f}
- **Total Tokens:** {total_tokens:,}
- **Avg Tokens/Question:** {total_tokens/total:.0f}

---

## üìã Question Results

| # | Question | Answer | Success | Iterations | Tools |
|---|----------|--------|---------|------------|-------|
"""
        
        for i, r in enumerate(results, 1):
            question = r.get('Question', '')[:50] + "..."
            answer = str(r.get('Pred_Answer', 'N/A'))[:30]
            success = "‚úÖ" if r.get('Success', False) else "‚ùå"
            iterations = r.get('Iterations', 0)
            tools = r.get('Tool_Calls', 0)
            
            summary += f"| {i} | {question} | {answer} | {success} | {iterations} | {tools} |\n"
        
        summary += "\n---\n\n**End of Summary Report**\n"
        
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(summary)
        
        return summary
